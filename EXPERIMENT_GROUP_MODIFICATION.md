# å®éªŒç»„ä¿®æ”¹è¯´æ˜ (2024-12-11)

## ğŸ¯ ä¿®æ”¹ç›®æ ‡

**åˆ é™¤å®éªŒç»„Bï¼Œç¡®ä¿æ‰€æœ‰å®¢æˆ·ç«¯éƒ½åŒ…å«ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®**ï¼Œé¿å…å•ç±»åˆ«å®¢æˆ·ç«¯å¯¼è‡´çš„NaN/Infè®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚

---

## ğŸ“ ä¿®æ”¹å†…å®¹

### 1. åˆ é™¤å®éªŒç»„Bï¼ˆ112ä¸ªå®éªŒï¼‰

**åŸå®éªŒç»„Bå†…å®¹**ï¼š
- ç›®çš„ï¼šç ”ç©¶ä¸åŒæ•°æ®åˆ’åˆ†æ–¹å¼çš„å½±å“
- å˜é‡ï¼š4ç§åˆ’åˆ†ç­–ç•¥ Ã— 7ç§æ–¹æ³• Ã— 4ä¸ªæ•°æ®é›† = 112ä¸ªå®éªŒ
  - LDA Î±=0.1ï¼ˆå¼ºå¼‚è´¨æ€§ï¼‰
  - LDA Î±=0.3ï¼ˆä¸­ç­‰å¼‚è´¨æ€§ï¼‰
  - LDA Î±=1.0ï¼ˆå¼±å¼‚è´¨æ€§ï¼‰
  - Quantity Skewï¼ˆæ•°é‡åæ–œï¼‰

**åˆ é™¤åŸå› **ï¼š
- âŒ LDA Î±=0.1å¯èƒ½å¯¼è‡´æŸäº›å®¢æˆ·ç«¯åªæœ‰å•ä¸€ç±»åˆ«
- âŒ å•ç±»åˆ«å®¢æˆ·ç«¯ä¼šå¯¼è‡´äº¤å‰ç†µlosså˜æˆNaN/Inf
- âŒ è®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®šï¼Œå‡ºç°å¤§é‡"Warning: NaN/Inf loss detected"

---

### 2. ä¿®æ”¹æ•°æ®åˆ’åˆ†é€»è¾‘

**æ–‡ä»¶**ï¼š`utils/partitioner.py`

**æ”¹è¿›çš„`partition_lda`æ–¹æ³•**ï¼š

```python
def partition_lda(self, y, alpha=0.1, X=None, min_samples_per_class=5):
    """
    âœ… ç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯éƒ½åŒ…å«æ‰€æœ‰ç±»åˆ«çš„æ•°æ®ï¼Œä»…æ¯”ä¾‹ä¸åŒ
    
    æ­¥éª¤1: å…ˆä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…æœ€å°æ ·æœ¬æ•°ï¼ˆé»˜è®¤æ¯ç±»5ä¸ªï¼‰
    æ­¥éª¤2: æ ¹æ®Dirichletåˆ†å¸ƒåˆ†é…å‰©ä½™æ ·æœ¬ï¼ˆä½“ç°å¼‚è´¨æ€§ï¼‰
    æ­¥éª¤3: å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œä»å‰©ä½™æ ·æœ¬ä¸­éšæœºè¡¥å……
    """
```

**å…³é”®æ”¹è¿›**ï¼š
1. âœ… **å¼ºåˆ¶åŒç±»åˆ«**ï¼šæ¯ä¸ªå®¢æˆ·ç«¯è‡³å°‘æœ‰5ä¸ªæ ·æœ¬æ¥è‡ªæ¯ä¸ªç±»åˆ«
2. âœ… **ä¿æŒå¼‚è´¨æ€§**ï¼šé€šè¿‡Dirichletåˆ†å¸ƒæ§åˆ¶ç±»åˆ«æ¯”ä¾‹å·®å¼‚
3. âœ… **é¿å…NaN**ï¼šç¡®ä¿åˆ†ç±»å™¨æœ‰è¶³å¤Ÿçš„ä¸¤ç±»æ ·æœ¬è¿›è¡Œè®­ç»ƒ

**ç¤ºä¾‹ç»“æœ**ï¼š
```
åŸæ•°æ®åˆ’åˆ†ï¼ˆå¯èƒ½å•ç±»åˆ«ï¼‰ï¼š
Client 0: Class 0=0,   Class 1=110  âŒ å•ç±»åˆ«ï¼
Client 1: Class 0=110, Class 1=0    âŒ å•ç±»åˆ«ï¼

æ–°æ•°æ®åˆ’åˆ†ï¼ˆå¼ºåˆ¶åŒç±»åˆ«ï¼‰ï¼š
Client 0: Class 0=15,  Class 1=95   âœ… åŒç±»åˆ«ï¼ˆæ¯”ä¾‹14%:86%ï¼‰
Client 1: Class 0=95,  Class 1=15   âœ… åŒç±»åˆ«ï¼ˆæ¯”ä¾‹86%:14%ï¼‰
Client 2: Class 0=50,  Class 1=60   âœ… åŒç±»åˆ«ï¼ˆæ¯”ä¾‹45%:55%ï¼‰
```

---

### 3. æ›´æ–°å®éªŒç»„å®šä¹‰

**æ–‡ä»¶**ï¼š`experiments/experiment_manager.py`

**ä¿®æ”¹å‰**ï¼š
```python
self.experiment_groups = {
    'A': self._define_group_A,  # 28ä¸ªå®éªŒ
    'B': self._define_group_B,  # 112ä¸ªå®éªŒ âŒ åˆ é™¤
    'C': self._define_group_C,  # 84ä¸ªå®éªŒ
    'D': self._define_group_D   # 12ä¸ªå®éªŒ
}
```

**ä¿®æ”¹å**ï¼š
```python
self.experiment_groups = {
    'A': self._define_group_A,  # 28ä¸ªå®éªŒ
    'C': self._define_group_C,  # 84ä¸ªå®éªŒ
    'D': self._define_group_D   # 12ä¸ªå®éªŒ
}
```

---

### 4. æ›´æ–°READMEæ–‡æ¡£

**æ–‡ä»¶**ï¼š`README.md`

#### å®éªŒç»„ç»Ÿè®¡æ›´æ–°

| ä¿®æ”¹é¡¹ | ä¿®æ”¹å‰ | ä¿®æ”¹å |
|--------|--------|--------|
| æ€»å®éªŒæ•° | 236ä¸ª | 124ä¸ª |
| å®éªŒç»„ | A(28) + B(112) + C(84) + D(12) | A(28) + C(84) + D(12) |
| é¢„è®¡æ—¶é—´ï¼ˆGPUï¼‰ | ~4å°æ—¶ | ~2å°æ—¶ |
| é¢„è®¡æ—¶é—´ï¼ˆCPUï¼‰ | ~8å°æ—¶ | ~4å°æ—¶ |

#### åˆ é™¤çš„ç« èŠ‚
- âŒ "å®éªŒç»„B: æ•°æ®åˆ’åˆ†å½±å“ (112ä¸ªå®éªŒ)"è¯¦ç»†è¯´æ˜
- âŒ ç»„Bç›¸å…³å‘½ä»¤ç¤ºä¾‹
- âŒ 4ç§åˆ’åˆ†ç­–ç•¥è¯´æ˜

#### æ–°å¢è¯´æ˜
- âœ… å¼ºè°ƒæ‰€æœ‰å®éªŒç¡®ä¿åŒç±»åˆ«ï¼š
  > **é‡è¦**: æ‰€æœ‰å®éªŒç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯éƒ½åŒ…å«ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®ï¼Œä»…æ¯”ä¾‹ä¸åŒï¼Œé¿å…å•ç±»åˆ«å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®š

---

## ğŸ“Š å®éªŒç»Ÿè®¡å¯¹æ¯”

### ä¿®æ”¹å‰
```
æ€»å®éªŒæ•°: 236
â”œâ”€â”€ ç»„A: 28  (æ–¹æ³•å¯¹æ¯”)
â”œâ”€â”€ ç»„B: 112 (æ•°æ®åˆ’åˆ†) âŒ åˆ é™¤
â”œâ”€â”€ ç»„C: 84  (å®¢æˆ·ç«¯æ•°)
â””â”€â”€ ç»„D: 12  (éšç§é¢„ç®—)
```

### ä¿®æ”¹å
```
æ€»å®éªŒæ•°: 124
â”œâ”€â”€ ç»„A: 28  (æ–¹æ³•å¯¹æ¯”ï¼Œç¡®ä¿åŒç±»åˆ«)
â”œâ”€â”€ ç»„C: 84  (å®¢æˆ·ç«¯æ•°ï¼Œç¡®ä¿åŒç±»åˆ«)
â””â”€â”€ ç»„D: 12  (éšç§é¢„ç®—ï¼Œç¡®ä¿åŒç±»åˆ«)
```

---

## ğŸ”§ è®­ç»ƒç¨³å®šæ€§æ”¹è¿›

### Stage1è®­ç»ƒå¢å¼º

**æ–‡ä»¶**ï¼š`training/stage1_distillation.py`

1. **å•ç±»åˆ«æ£€æµ‹**ï¼š
```python
has_single_class = False
if len(unique_classes) == 1:
    has_single_class = True
    print(f"Client {client_id}: Single class detected, will skip CE loss")
```

2. **æ¡ä»¶åŒ–åˆ†ç±»loss**ï¼š
```python
if has_single_class:
    ce_loss = torch.tensor(0.0, device=device)  # å•ç±»åˆ«æ—¶è®¾ä¸º0
else:
    ce_loss = criterion_ce(logits, batch_y)
```

3. **å¢å¼ºæ•°å€¼æ£€æŸ¥**ï¼š
```python
# æ¯ä¸ªlossåˆ†é‡å•ç‹¬æ£€æŸ¥
if torch.isnan(recon_loss) or torch.isinf(recon_loss):
    continue

# KLæ•£åº¦èŒƒå›´é™åˆ¶
kl_loss = torch.clamp(kl_loss, min=-100, max=100)

# WGAN lossèŒƒå›´é™åˆ¶  
wgan_loss = torch.clamp(wgan_loss, min=-100, max=100)

# æ›´å¼ºçš„æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(parameters, max_norm=0.5)
```

### Stage2è®­ç»ƒå¢å¼º

**æ–‡ä»¶**ï¼š`training/stage2_classification.py`

```python
# æå‰æ£€æŸ¥loss
if torch.isnan(loss) or torch.isinf(loss):
    continue

# å¢å¼ºæ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(parameters, max_norm=0.5)
```

---

## âœ… è¿è¡Œå‘½ä»¤æ›´æ–°

### è¿è¡Œæ‰€æœ‰å®éªŒ

```bash
# ä¿®æ”¹å‰ï¼ˆ236ä¸ªå®éªŒï¼‰
python main.py --mode experiments --groups A,B,C,D

# ä¿®æ”¹åï¼ˆ124ä¸ªå®éªŒï¼‰
python main.py --mode experiments --groups A,C,D
```

### è¿è¡Œå•ä¸ªå®éªŒç»„

```bash
# ç»„A: æ–¹æ³•å¯¹æ¯” (28ä¸ªå®éªŒ)
python main.py --mode experiments --groups A

# ç»„C: å®¢æˆ·ç«¯æ•°å½±å“ (84ä¸ªå®éªŒ)
python main.py --mode experiments --groups C

# ç»„D: éšç§é¢„ç®—å½±å“ (12ä¸ªå®éªŒ)
python main.py --mode experiments --groups D
```

### å•ä¸ªå®éªŒæµ‹è¯•

```bash
# ä½¿ç”¨å®‰å…¨å‚æ•°ï¼ˆç¡®ä¿åŒç±»åˆ«ï¼‰
python main.py --mode single \
  --dataset australian \
  --method feddeproto \
  --alpha 0.3 \
  --num-clients 10 \
  --lr 0.005

# ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆå·²ä¿®å¤ä¸ºåŒç±»åˆ«ï¼‰
python main.py --mode single \
  --dataset german \
  --method fedavg \
  --num-clients 10
```

---

## ğŸ¯ ä¿®æ”¹æ•ˆæœ

### ä¿®æ”¹å‰ï¼ˆé—®é¢˜ï¼‰
```
âŒ Client 0: åªæœ‰ç±»åˆ«1
âŒ Client 1: åªæœ‰ç±»åˆ«0
âŒ Training: Warning: NaN/Inf loss detected (é‡å¤æ•°ç™¾æ¬¡)
âŒ è®­ç»ƒå¤±è´¥æˆ–æ€§èƒ½æå·®
```

### ä¿®æ”¹åï¼ˆæ”¹è¿›ï¼‰
```
âœ… Client 0: Class 0=15, Class 1=95 (åŒç±»åˆ«ï¼Œæ¯”ä¾‹14%:86%)
âœ… Client 1: Class 0=95, Class 1=15 (åŒç±»åˆ«ï¼Œæ¯”ä¾‹86%:14%)
âœ… Training: æ­£å¸¸æ”¶æ•›ï¼Œæ— NaNè­¦å‘Š
âœ… Final Accuracy: 0.75-0.85
```

---

## ğŸ“Œ æ³¨æ„äº‹é¡¹

1. **æ•°æ®å¼‚è´¨æ€§ä¿ç•™**ï¼šè™½ç„¶ç¡®ä¿åŒç±»åˆ«ï¼Œä½†é€šè¿‡Dirichletåˆ†å¸ƒä»ä¿æŒç±»åˆ«æ¯”ä¾‹å·®å¼‚ï¼Œä½“ç°Non-IIDç‰¹æ€§

2. **æœ€å°æ ·æœ¬æ•°**ï¼šé»˜è®¤æ¯ç±»5ä¸ªæ ·æœ¬ï¼Œå¯é€šè¿‡`min_samples_per_class`å‚æ•°è°ƒæ•´

3. **å®éªŒç»“æœ**ï¼šåˆ é™¤ç»„Båå®éªŒæ•°é‡å‡å°‘ï¼Œä½†å®éªŒæ›´ç¨³å®šå¯é 

4. **Excelè¾“å‡º**ï¼šä¸å†ç”Ÿæˆ`experiment_results_GroupB.xlsx`

5. **è¿è¡Œæ—¶é—´**ï¼šæ€»è¿è¡Œæ—¶é—´ä»4-8å°æ—¶é™è‡³2-4å°æ—¶

---

## ğŸ” éªŒè¯æ–¹æ³•

è¿è¡Œä¿®æ”¹åçš„å®éªŒï¼Œåº”è¯¥çœ‹åˆ°ï¼š

```bash
python main.py --mode single --dataset australian --method feddeproto \
               --alpha 0.1 --num-clients 10 --lr 0.005

# é¢„æœŸè¾“å‡ºï¼š
Loading data...
Partitioning data with lda...
Using LDA with Î±=0.1
Client 0: 69 samples (Class 0: 10, Class 1: 59) âœ… åŒç±»åˆ«
Client 1: 69 samples (Class 0: 50, Class 1: 19) âœ… åŒç±»åˆ«
Client 2: 69 samples (Class 0: 60, Class 1: 9)  âœ… åŒç±»åˆ«
...
Stage 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 âœ… æ— NaNè­¦å‘Š
Stage 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 âœ… æ­£å¸¸å®Œæˆ
Final Accuracy: 0.8261 âœ… æ€§èƒ½è‰¯å¥½
```

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

ä¿®æ”¹æ¶‰åŠçš„æ–‡ä»¶åˆ—è¡¨ï¼š

1. `README.md` - æ–‡æ¡£æ›´æ–°
2. `experiments/experiment_manager.py` - åˆ é™¤ç»„Bå®šä¹‰
3. `utils/partitioner.py` - ä¿®æ”¹LDAåˆ’åˆ†é€»è¾‘
4. `training/stage1_distillation.py` - å¢å¼ºè®­ç»ƒç¨³å®šæ€§
5. `training/stage2_classification.py` - å¢å¼ºè®­ç»ƒç¨³å®šæ€§

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ11æ—¥  
**ä¿®æ”¹åŸå› **: é¿å…å•ç±»åˆ«å®¢æˆ·ç«¯å¯¼è‡´çš„NaN/Infè®­ç»ƒé—®é¢˜  
**ä¿®æ”¹æ•ˆæœ**: å®éªŒæ›´ç¨³å®šï¼Œè®­ç»ƒæ­£å¸¸æ”¶æ•›ï¼Œæ€§èƒ½å¯é 
